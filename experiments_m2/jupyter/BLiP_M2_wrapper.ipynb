{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure paths for proper import \n",
    "import sys\n",
    "import os\n",
    "file_dir = os.getcwd()\n",
    "package_dir = os.path.split(os.path.split(file_dir)[0])[0]\n",
    "sys.path.insert(0, package_dir)\n",
    "\n",
    "# Data file\n",
    "output_dir = package_dir + \"/blip_wrapper\"\n",
    "sys.path.insert(0, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import copy\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import deblending_runjingdev.simulated_datasets_lib as simulated_datasets_lib\n",
    "import deblending_runjingdev.sdss_dataset_lib as sdss_dataset_lib\n",
    "import deblending_runjingdev.starnet_lib as starnet_lib\n",
    "import deblending_runjingdev.sleep_lib as sleep_lib\n",
    "import deblending_runjingdev.image_statistics_lib as image_statistics_lib\n",
    "import deblending_runjingdev.daophot_catalog_lib as daophot_catalog_lib\n",
    "\n",
    "import deblending_runjingdev.plotting_utils as plotting_utils\n",
    "\n",
    "from deblending_runjingdev.which_device import device\n",
    "\n",
    "np.random.seed(34534)\n",
    "_ = torch.manual_seed(94219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blip_dir = os.path.split(os.path.split(package_dir)[0])[0] + \"/pyblip\"\n",
    "#sys.path.insert(0, blip_dir)\n",
    "import pyblip\n",
    "import networkx as nx\n",
    "print(pyblip.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blip-wrappers\n",
    "import performance_eval as pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load SDSS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_image, sdss_background, hubble_locs, hubble_fluxes, sdss_data, wcs = \\\n",
    "    sdss_dataset_lib.load_m2_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nelec_per_nmgy = sdss_data[0]['nelec_per_nmgy'][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the subimage of M2 considered in our paper\n",
    "plt.matshow(sdss_image[0].cpu(), cmap = plt.cm.gray)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a few example patches. \n",
    "# blue are hubble locations\n",
    "\n",
    "f, axarr = plt.subplots(1, 5, figsize=(12, 6))\n",
    "\n",
    "for i in range(5): \n",
    "    plotting_utils.plot_subimage(axarr[i], \n",
    "                                 sdss_image[0],\n",
    "                                 None, \n",
    "                                 hubble_locs, \n",
    "                                 x0 = int(np.random.choice(100, 1)), \n",
    "                                 x1 = int(np.random.choice(100, 1)), \n",
    "                                 patch_slen = 7)\n",
    "    axarr[i].set_xticks([]);\n",
    "    axarr[i].set_yticks([]);\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load StarNet fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wake-sleep encoder\n",
    "star_encoder_ws = starnet_lib.StarEncoder(slen = 100,\n",
    "                                            ptile_slen = 8,\n",
    "                                            step = 2,\n",
    "                                            edge_padding = 3, \n",
    "                                            n_bands = 2,\n",
    "                                            max_detections = 2)\n",
    "\n",
    "star_encoder_ws.load_state_dict(torch.load('../fits/starnet-encoder-iter2', \n",
    "                               map_location=lambda storage, loc: storage))\n",
    "\n",
    "\n",
    "star_encoder_ws.eval(); \n",
    "star_encoder_ws.to(device);\n",
    "\n",
    "starnet_ws_map_locs, starnet_ws_map_fluxes, starnet_ws_map_n_stars = \\\n",
    "    star_encoder_ws.sample_star_encoder(sdss_image.unsqueeze(0).to(device), \n",
    "                                    return_map_n_stars = True, \n",
    "                                    return_map_star_params = True)[0:3]\n",
    "\n",
    "# Sample from the posterior\n",
    "n_starnet_samples = 10000\n",
    "starnet_ws_locs, starnet_ws_fluxes, starnet_ws_map_n_stars = \\\n",
    "    star_encoder_ws.sample_star_encoder(sdss_image.unsqueeze(0).to(device), \n",
    "                                        n_samples=n_starnet_samples,\n",
    "                                        return_map_n_stars = False, \n",
    "                                        return_map_star_params = False\n",
    "                                       )[0:3]\n",
    "starnet_ws_locs = starnet_ws_locs[0::10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analysis start to finish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_loc = hubble_locs.numpy()\n",
    "hub_flux = hubble_fluxes[:, 0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slen = 100\n",
    "blip_out = dict()\n",
    "qs = np.arange(1, 16) / 20\n",
    "grid_sizes = np.around(np.logspace(np.log10(50), 4, 25))\n",
    "print(\"The number of candidate groups is\", int(np.sum(grid_sizes**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cg_to_dict(cg):\n",
    "    \"\"\" For saving cand groups \"\"\"\n",
    "    out = dict()\n",
    "    out['group'] = list(cg.group)\n",
    "    out['pep'] = cg.pep\n",
    "    for key in cg.data:\n",
    "        if key == 'blip-group':\n",
    "            continue\n",
    "        if isinstance(cg.data[key], set):\n",
    "            out[key] = list(cg.data[key])\n",
    "        elif isinstance(cg.data[key], np.float32):\n",
    "            out[key] = float(cg.data[key])\n",
    "        else:\n",
    "            out[key] = cg.data[key]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recompute_cand_groups = False\n",
    "max_pep = 0.75\n",
    "\n",
    "if recompute_cand_groups:\n",
    "    peps = pyblip.create_groups_cts.grid_peps(\n",
    "        starnet_ws_locs.numpy(),\n",
    "        grid_sizes,\n",
    "        extra_centers=starnet_ws_map_locs.numpy()[0],\n",
    "        max_pep=max_pep,\n",
    "        log_interval=10, \n",
    "        shape='circle',\n",
    "        count_signals=True,\n",
    "    )\n",
    "    all_cgs = dict()\n",
    "    for count_signals in [True, False]:\n",
    "        if count_signals:\n",
    "            input_peps = peps\n",
    "        else:\n",
    "            input_peps = {}\n",
    "            for key in peps.keys():\n",
    "                input_peps[key] = 1 - peps[key]['pip']\n",
    "        \n",
    "        # Create cand groups\n",
    "        all_cand_groups, _ = pyblip.create_groups_cts.grid_peps_to_cand_groups(\n",
    "            input_peps, \n",
    "            max_blip_size=5000, \n",
    "            shape='circle', \n",
    "            verbose=True,\n",
    "            max_pep=max_pep,\n",
    "        )\n",
    "        # Save\n",
    "        cgs_saveable = [[] for j in range(len(all_cand_groups))]\n",
    "        for j, cgs in enumerate(all_cand_groups):\n",
    "            cgs_saveable[j].extend(\n",
    "                [cg_to_dict(x) for x in cgs]\n",
    "            )\n",
    "        all_cgs[int(count_signals)] = cgs_saveable\n",
    "        with open(f\"{output_dir}/m2_cand_groups.json\", 'w') as file:\n",
    "            file.write(json.dumps(all_cgs))\n",
    "    \n",
    "# Load cached cand_groups\n",
    "with open(f\"{output_dir}/m2_cand_groups.json\", 'r') as file:\n",
    "    all_cand_groups_cached = json.load(file)\n",
    "\n",
    "all_cand_groups_dict = dict()\n",
    "for count_signals in [0, 1]:\n",
    "    all_cand_groups = [[] for _ in range(len(\n",
    "        all_cand_groups_cached[str(count_signals)]\n",
    "    ))]\n",
    "    for j, cgs in enumerate(all_cand_groups_cached[str(count_signals)]):\n",
    "        for data_dict in cgs:\n",
    "            all_cand_groups[j].append(\n",
    "                pyblip.create_groups.CandidateGroup(\n",
    "                    group=data_dict['group'],\n",
    "                    pep=data_dict['pep'],\n",
    "                    data=data_dict\n",
    "                )\n",
    "            )\n",
    "    all_cand_groups_dict[count_signals] = all_cand_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight functions\n",
    "from pyblip.weight_fns import inverse_radius_weight as inv_rad_weight\n",
    "\n",
    "# Create a few weight functions\n",
    "def softmax_inv_radius(cg, t=0.01):\n",
    "    exprad = np.exp(t / cg.data['radius'])\n",
    "    if exprad > 1e10:\n",
    "        return 1\n",
    "    return exprad / (1.0 + exprad)\n",
    "    \n",
    "def const_weight(cg):\n",
    "    return 1.0\n",
    "\n",
    "def inv_ci(cg):\n",
    "    return 1 / len(cg.data['nsignals'])\n",
    "\n",
    "weight_fn_dict = {\n",
    "    #\"const (fixed res)\":const_weight,\n",
    "    #\"soft_inv_rad\":softmax_inv_radius,\n",
    "    \"inv_rad\":inv_rad_weight,\n",
    "    #\"const\":const_weight,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recompute_blip = False\n",
    "\n",
    "if recompute_blip:\n",
    "\n",
    "    time0 = time.time()\n",
    "    blip_out = dict()\n",
    "\n",
    "    for count_signals, weight_fn in zip(\n",
    "        [False, True], \n",
    "        [inv_rad_weight, inv_ci]\n",
    "    ):\n",
    "        blip_out[int(count_signals)] = dict()\n",
    "\n",
    "        # Run pyblip to control FDR\n",
    "        for q in qs:\n",
    "            all_rej = []\n",
    "            print(f\"At q={q}, count_signals={count_signals}, at {elapsed(time0)}.\")\n",
    "            for cand_groups in all_cand_groups_dict[int(count_signals)]:\n",
    "                cgs = [copy.deepcopy(n) for n in cand_groups]\n",
    "                rej = pyblip.blip.BLiP(\n",
    "                    cand_groups=cgs,\n",
    "                    weight_fn=weight_fn,\n",
    "                    max_pep=max(2*q, 0.5),\n",
    "                    error='fdr',\n",
    "                    q=q,\n",
    "                    verbose=True,\n",
    "                    deterministic=False,\n",
    "                )\n",
    "                all_rej.extend(rej)\n",
    "\n",
    "            blip_out[int(count_signals)][q] = all_rej\n",
    "\n",
    "    # Make sure output is hashable\n",
    "    blip_out_saveable = dict()\n",
    "    for key in blip_out:\n",
    "        blip_out_saveable[key] = {q:[] for q in blip_out[key].keys()}\n",
    "        for q in blip_out[key]:\n",
    "            blip_out_saveable[key][q] = [cg_to_dict(x) for x in blip_out[key][q]]\n",
    "    # Save output and cand_groups\n",
    "    with open(f\"{output_dir}/m2_blip_output.json\", 'w') as file:\n",
    "        file.write(json.dumps(blip_out_saveable))\n",
    "        \n",
    "# Load output\n",
    "with open(f\"{output_dir}/m2_blip_output.json\", 'r') as file:\n",
    "    blip_out_read = json.load(file)\n",
    "\n",
    "# Turn into nodes\n",
    "def try_float(x):\n",
    "    try:\n",
    "        return np.around(float(x), 4)\n",
    "    except:\n",
    "        return x\n",
    "    \n",
    "# Turn into nodes\n",
    "blip_out = dict()\n",
    "for weight_fn_name in blip_out_read.keys():\n",
    "    blip_out[weight_fn_name] = {try_float(q):[] for q in blip_out_read[weight_fn_name].keys()}\n",
    "    for q in blip_out_read[weight_fn_name]:\n",
    "        for data_dict in blip_out_read[weight_fn_name][q]:\n",
    "            blip_out[weight_fn_name][try_float(q)].append(\n",
    "                pyblip.create_groups.CandidateGroup(\n",
    "                    group=data_dict['group'],\n",
    "                    pep=data_dict['pep'],\n",
    "                    data=data_dict\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(a) Evaluation for BLiP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_slack = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blip_power_outputs = []\n",
    "all_estimators = []\n",
    "power_cols = ['method', 'count_signals', 'q', 'power', 'fdr', 'res_power', 'fixed_radius']\n",
    "estimator_columns = [\n",
    "    'method', 'method2', 'x', 'y', 'radius', 'true_disc', 'min_dist'\n",
    "]\n",
    "\n",
    "for count_signals in blip_out.keys():\n",
    "    for q in blip_out[count_signals].keys():\n",
    "        # Calculate power + FDR for blip output\n",
    "        locs_est, locs_error, locs_peps, weights = pe.blip_output_to_catalogue(\n",
    "            blip_out[count_signals][q]\n",
    "        )\n",
    "        # Create nsignal ci if we are counting signals\n",
    "        if count_signals == '1':\n",
    "            nsignal_ci = [x.data['nsignals'] for x in blip_out[count_signals][q]]\n",
    "        else:\n",
    "            nsignal_ci = None\n",
    "            \n",
    "        power, fdr, res_power, ppv_bool = pe.catalogue_power_fdr(\n",
    "            locs_true=hub_loc,\n",
    "            locs_est=locs_est,\n",
    "            locs_error=locs_error,\n",
    "            weights=weights,\n",
    "            slen=slen,\n",
    "            pad=0,\n",
    "            slack=global_slack,\n",
    "            return_bools=True,\n",
    "            nsignal_ci=nsignal_ci,\n",
    "        )\n",
    "        blip_power_outputs.append([\n",
    "            'Starnet + BLiP', count_signals, q, power, fdr, res_power, 0\n",
    "        ])\n",
    "\n",
    "        # Calculate distance-based metric\n",
    "        d = hub_loc.shape[1]\n",
    "        m = hub_loc.shape[0]\n",
    "        n = locs_est.shape[0]\n",
    "\n",
    "        # Save estimators\n",
    "        if int(count_signals) == 0:\n",
    "            est_df = pd.DataFrame(columns=estimator_columns)\n",
    "            est_df['y'] = locs_est[:, 0]\n",
    "            est_df['x'] = locs_est[:, 1]\n",
    "            est_df['radius'] = locs_error\n",
    "            est_df['true_disc'] = ppv_bool\n",
    "            est_df['method'] = 'Starnet + BLiP'\n",
    "            est_df['method2'] = f'BLiP (q={q})'\n",
    "            all_estimators.append(est_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(b) Evaluation of power for MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Performance evaluation for map estimation with different slack parameters\n",
    "fixed_radii = [1/1600, 1/1200, 1/800, 1/600, 1/400, 1/300, 1/200, 1/150, 1/100, 1/50,]\n",
    "map_ests = starnet_ws_map_locs.numpy()[0]\n",
    "map_power_outputs = []\n",
    "\n",
    "for count_signals in [True, False]:\n",
    "    for radius in fixed_radii:\n",
    "        nsignal_ci = [set([1]) for _ in range(len(map_ests))] if count_signals else None\n",
    "        power, fdr, res_power = pe.catalogue_power_fdr(\n",
    "            locs_true=hub_loc,\n",
    "            locs_est=map_ests,\n",
    "            locs_error=radius * np.ones(map_ests.shape[0]),\n",
    "            nsignal_ci=nsignal_ci,\n",
    "            slen=slen,\n",
    "            pad=0,\n",
    "            slack=global_slack\n",
    "        )\n",
    "        map_power_outputs.append([\n",
    "            'Starnet (MAP)', count_signals, 1, power, fdr, res_power, radius\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "power_outputs = map_power_outputs + blip_power_outputs\n",
    "pdf = pd.DataFrame(power_outputs, columns=power_cols)\n",
    "pdf['count_signals'] = pdf['count_signals'].astype(int)\n",
    "pdf.to_csv(f\"{output_dir}/m2_power_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(c), dataframe of the output for MAP and then save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance from points to sources for map estimates\n",
    "m = hub_loc.shape[0]\n",
    "n = map_ests.shape[0]\n",
    "min_dists = np.abs(\n",
    "    hub_loc.reshape(m, 1, 2) - map_ests.reshape(1, n, 2)\n",
    ").max(axis=2).min(axis=0)\n",
    "# map_dist_df = pd.DataFrame(columns=dist_cols)\n",
    "# map_dist_df['dist'] = min_dists\n",
    "# map_dist_df['method'] = 'Starnet (MAP)'\n",
    "# map_dist_df['q'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe of estimators for map\n",
    "other_est_dfs = []\n",
    "for count_signals in [False]:\n",
    "    for locs, true_disc, method, method2, mdists in zip(\n",
    "        [map_ests, hub_loc], \n",
    "        [min_dists <= 0.005, 1],\n",
    "        ['Starnet (MAP)', 'Hubble'],\n",
    "        ['MAP', 'Hubble'],\n",
    "        [min_dists, 0]\n",
    "    ):\n",
    "        est_df = pd.DataFrame(columns=estimator_columns)\n",
    "        est_df['y'] = locs[:, 0]\n",
    "        est_df['x'] = locs[:, 1]\n",
    "        est_df['radius'] = 0\n",
    "        est_df['true_disc'] = true_disc\n",
    "        #est_df['count_signals'] = count_signals\n",
    "        est_df['method'] = method\n",
    "        est_df['method2'] = method2\n",
    "        est_df['min_dist'] = mdists\n",
    "        other_est_dfs.append(est_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ests = pd.concat(all_estimators + other_est_dfs, axis='index')\n",
    "all_ests.to_csv(f\"{output_dir}/m2_ests_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rect_flags(data, lx, ux, ly, uy):\n",
    "    \"\"\"\n",
    "    Subset data to just look at a specific rectangle\n",
    "    \"\"\"\n",
    "    flags = (data[:, 0] >= lx)\n",
    "    flags = flags & (data[:, 1] >= ly)\n",
    "    flags = flags & (data[:, 0] <= ux)\n",
    "    flags = flags & (data[:, 1] <= uy)\n",
    "    return flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locs_to_hist_inputs(samples, nbins):\n",
    "    \"\"\" No double counting per square \"\"\"\n",
    "    N1, N2, d = samples.shape\n",
    "    output = []\n",
    "    for i in range(N1):\n",
    "        subset = samples[i]\n",
    "        subset = subset[np.any(subset != 0, axis=-1)]\n",
    "        corners = np.floor(subset * nbins)\n",
    "        corners = corners.astype(float) / nbins\n",
    "        centers = corners + 1 / (2 * nbins)\n",
    "        output.append(np.unique(centers, axis=0))\n",
    "    return np.concatenate(output, axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "lv = 0.4\n",
    "uv = 0.6\n",
    "nbins = 50\n",
    "\n",
    "snet_locs = locs_to_hist_inputs(\n",
    "    starnet_ws_locs.numpy().copy(), nbins=int(nbins / (uv - lv))\n",
    ")\n",
    "map_locs = map_ests.reshape(-1, 2)\n",
    "\n",
    "flags = get_rect_flags(snet_locs, lv, uv, lv, uv)\n",
    "map_flags = get_rect_flags(map_locs, lv, uv, lv, uv)\n",
    "hub_flags = get_rect_flags(hub_loc, lv, uv, lv, uv)\n",
    "\n",
    "fig = px.density_heatmap(\n",
    "    x=snet_locs[flags, 1], \n",
    "    y=snet_locs[flags, 0],\n",
    "    color_continuous_scale='blues', # also 'cividis'\n",
    "    height=800,\n",
    "    width=800,\n",
    "    nbinsx=nbins,\n",
    "    nbinsy=nbins,\n",
    ")\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=map_locs[map_flags, 1], \n",
    "    y=map_locs[map_flags, 0],\n",
    "    showlegend=False,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        symbol='x',\n",
    "        opacity=0.5,\n",
    "        color='red'\n",
    "    )\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=hub_loc[hub_flags, 1], \n",
    "    y=hub_loc[hub_flags, 0],\n",
    "    showlegend=False,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        symbol='x',\n",
    "        opacity=0.5,\n",
    "        color='black'\n",
    "    )\n",
    "))\n",
    "\n",
    "# Add discoveries\n",
    "for cg in blip_out['0'][0.05]:\n",
    "    if cg.data['dim0'] <= uv and cg.data['dim0'] >= lv:\n",
    "        if cg.data['dim1'] <= uv and cg.data['dim1'] >= lv:\n",
    "            if True:#cg.pep < 0.5 and cg.pep > 0.2:\n",
    "                fig.add_shape(type='circle',\n",
    "                    xref='x', yref='y',\n",
    "                    x0=cg.data['dim1'] - cg.data['radius'],\n",
    "                    y0=cg.data['dim0'] - cg.data['radius'],\n",
    "                    x1=cg.data['dim1'] + cg.data['radius'],\n",
    "                    y1=cg.data['dim0'] + cg.data['radius'],\n",
    "                    line_color='black',\n",
    "                )  \n",
    "                \n",
    "    \n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.density_heatmap(\n",
    "    x=snet_locs[flags, 1], \n",
    "    y=snet_locs[flags, 0],\n",
    "    color_continuous_scale='blues',\n",
    "    height=800,\n",
    "    width=800,\n",
    "    nbinsx=nbins,\n",
    "    nbinsy=nbins,\n",
    ")\n",
    "#fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
